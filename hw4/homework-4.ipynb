{
 "metadata": {
  "name": "",
  "signature": "sha256:21fb34c00efdb2f84f7ba7e0f07e80ec0635612d946940ae7cb3ee10ac3ffbe4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\u0414\u043e\u043c\u0430\u0448\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435 4. \u041a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438\u0437 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u0421\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041f\u0435\u0440\u0432\u044b\u0439 \u044d\u0442\u0430\u043f - \u0441\u0431\u043e\u0440 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0442\u044c\u0441\u044f \u043a Twitter API \u0438 \u0437\u0430\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u0442\u044c \u0442\u0432\u0438\u0442\u044b \u043f\u043e id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f. \n",
      "\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u043a API \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e \u043e\u043f\u0438\u0441\u0430\u043d\u043e \u0432 \u0414\u0417 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "import nltk\n",
      "import json\n",
      "import pickle\n",
      "import re\n",
      "import time\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from collections import defaultdict\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from pytagcloud import create_tag_image\n",
      "\n",
      "CONSUMER_KEY = \"obfD8QyAByoGZRjgd1GeSKdUC\"\n",
      "CONSUMER_SECRET = \"zqup2Sk9Sb5DWm7q8swBgXaVeW1TncacBb0hfPbywyNDmknAcH\"\n",
      "\n",
      "ACCESS_TOKEN_KEY = \"103516608-CJ63nEox8ItY9DgDhYU2TBFeLnsZvYUUxslB1d3e\"\n",
      "ACCESS_TOKEN_SECRET = \"65XCfoYcZFUbYRPyNfV5Tso2f8Kv1yi2LYF3QV2xcloFP\"\n",
      "\n",
      "TRAINING_SET_URL = \"https://kaggle2.blob.core.windows.net/competitions-data/inclass/4277/twitter_train.txt?sv=2012-02-12&se=2015-04-19T13%3A31%3A49Z&sr=b&sp=r&sig=%2F37RhYx6edDa3cTP%2FRSdEY4bnIG1VPWR72tmrhipE6g%3D\"\n",
      "\n",
      "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
      "                  consumer_secret=CONSUMER_SECRET, \n",
      "                  access_token_key=ACCESS_TOKEN_KEY, \n",
      "                  access_token_secret=ACCESS_TOKEN_SECRET)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_users = pd.read_csv(TRAINING_SET_URL, sep=\",\", header=0, names=[\"user_id\", \"class\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u043c\u0435\u0442\u043e\u0434 GetUserTimeline \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 python-twitter. \u041e\u043d \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043d\u0435 \u0431\u043e\u043b\u0435\u0435 200 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.\n",
      "\n",
      "\u041c\u0435\u0442\u043e\u0434 \u0438\u043c\u0435\u0435\u0442 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u043f\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0443. \u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043a\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u0434\u043e\u0436\u0434\u0430\u0442\u044c \u0434\u043b\u044f \u043f\u043e\u0432\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u044f \u043a API \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u043c\u0435\u0442\u043e\u0434 `GetSleepTime`. \u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e\u0431 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 `GetUserTimeLine` \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u044b\u0437\u044b\u0432\u0430\u0442\u044c `GetSleepTime` \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c \"statuses/user_timeline\".\n",
      "\n",
      "\u041c\u0435\u0442\u043e\u0434 GetUserTimeline \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0442\u0438\u043f\u0430 Status. \u0423 \u044d\u0442\u0438\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0435\u0441\u0442\u044c \u043c\u0435\u0442\u043e\u0434 AsDict, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0442\u0432\u0438\u0442 \u0432 \u0432\u0438\u0434\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u044f.\n",
      "\n",
      "Id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0438\u0437 \u0444\u0430\u0439\u043b\u0430, \u043a\u0430\u043a \u0431\u044b\u043b\u043e \u0441\u0434\u0435\u043b\u0430\u043d\u043e \u0432 \u0414\u0417 1.\n",
      "\n",
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e `get_user_tweets(user_id)`. \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438\u0437 \u0444\u0430\u0439\u043b\u0430. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0442\u0432\u0438\u0442\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f, \u0433\u0434\u0435 \u043a\u0430\u0436\u0434\u044b\u0439 \u0442\u0432\u0438\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d \u0432 \u0432\u0438\u0434\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u044f. \u041f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442\u0441\u044f \u0432 \u0442\u0432\u0438\u0442\u0430\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043d\u0430\u043f\u0438\u0441\u0430\u043b \u0441\u0430\u043c. \u042d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u043e\u0442\u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0442\u0432\u0435\u0442\u044b \u0434\u0440\u0443\u0433\u0438\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c, \u0441\u0441\u044b\u043b\u043a\u0438 \u0438 \u0440\u0435\u0442\u0432\u0438\u0442\u044b, \u0430 \u0442\u0430\u043a \u0436\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0438 \u0432\u0438\u0434\u0435\u043e, \u0442\u0430\u043a \u043a\u0430\u043a \u043d\u0430\u0448\u0430 \u0446\u0435\u043b\u044c - \u043d\u0430\u0439\u0442\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_user_tweets(user_id):\n",
      "    # returns list of tweets (without retweets)\n",
      "    \n",
      "    try:\n",
      "        status = api.GetUserTimeline(user_id, count=200, trim_user=1, include_rts=0)\n",
      "        tweets_dict = [status[i].AsDict() for i in xrange(len(status))]\n",
      "        status = [tweets_dict[i][\"text\"] for i in xrange(len(tweets_dict))]\n",
      "    except twitter.TwitterError:\n",
      "        status = []\n",
      "    \n",
      "    return status"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "tweets = [None] * len(df_users[\"user_id\"])\n",
      "file_name = \"raw_tweets\"\n",
      "file_ext  = \".txt\"\n",
      "\n",
      "for i, user_id in enumerate(df_users[\"user_id\"]):\n",
      "    \n",
      "    try:\n",
      "        sleep_time = api.GetSleepTime(\"statuses/user_timeline\")\n",
      "    except twitter.TwitterError:\n",
      "        sleep_time = 900\n",
      "        \n",
      "    if sleep_time > 0:\n",
      "        print sleep_time + 1\n",
      "        time.sleep(sleep_time + 1)\n",
      "        print \"Wake up!\"\n",
      "    \n",
      "    tweets[i] = get_user_tweets(user_id)\n",
      "    \n",
      "    if i % 100 == 0:\n",
      "        print i, \" users loaded...\"\n",
      "        file_path = file_name + str(i) + file_ext\n",
      "        pickle.dump(tweets, open(file_path, \"wb\"))\n",
      "        \n",
      "pickle.dump(tweets, open(\"../files/raw_tweets_full.txt\", \"wb\"))\n",
      "\"\"\"\n",
      "\n",
      "tweets = pickle.loads(open(\"../files/raw_tweets_full.txt\", \"rb\").read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u0420\u0430\u0437\u0431\u043e\u0440 \u0442\u0435\u043a\u0441\u0442\u0430 \u0442\u0432\u0438\u0442\u0430"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u0442 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b - \u043f\u0430\u0440\u0430\u0433\u0440\u0430\u0444\u044b, \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0441\u043b\u043e\u0432\u0430. \u041c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u0442\u0432\u0438\u0442\u0430 \u043a \u0441\u043b\u043e\u0432\u0430\u043c. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0442\u0435\u043a\u0441\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u043d\u0430 \u0441\u043b\u043e\u0432\u0430. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e\u0433\u043e \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f.\n",
      "\n",
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e, `get_words(text)`. \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u0441\u0442\u0440\u043e\u043a\u0430 \u0441 \u0442\u0435\u043a\u0441\u0442\u043e\u043c. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0441\u0442\u0440\u043e\u043a (\u0441\u043b\u043e\u0432). \u041e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0443\u0447\u0435\u0441\u0442\u044c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0435 \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u0438 \u0438 \u0432\u044b\u0434\u0435\u043b\u0438\u0442\u044c \u043f\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u0430. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_words(text):\n",
      "    # Returns list of words from text\n",
      "    \n",
      "    return text.split()\n",
      "\n",
      "print get_words(\"Here are different words!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Here', 'are', 'different', 'words!']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0414\u0430\u043b\u0435\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435. \u0422\u043e \u0435\u0441\u0442\u044c \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0438\u0445 \u043a \u0444\u043e\u0440\u043c\u0435 \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0435\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438 \u043f\u0440. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 nltk. \u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043f\u043e \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0435, \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0438 \u043f\u0440\u0438\u043c\u0435\u0440\u0430\u0445 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u043d\u0430 \u0441\u0430\u0439\u0442\u0435 http://www.nltk.org/\n",
      "\n",
      "\u0414\u043b\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0432\u0441\u0435\u0445 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0441\u043b\u043e\u0432\u0430\u0440\u0435\u0439 \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c download \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 nltk."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0414\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441\u043b\u043e\u0432\u0430 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u044b \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443. \n",
      "\n",
      "\u0414\u043b\u044f \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c `WordNetLemmatizer` \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 nltk. \u0423 \u044d\u0442\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0435\u0441\u0442\u044c \u043c\u0435\u0442\u043e\u0434 `lemmatize`.\n",
      "\n",
      "\u0422\u0430\u043a\u0436\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0443\u0431\u0440\u0430\u0442\u044c \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430 \u0442\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u044b\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430. \u042d\u0442\u043e \u0447\u0430\u0441\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0435 \u0441\u043b\u043e\u0432\u0430, \u043d\u0435 \u043d\u0435\u0441\u0443\u0449\u0438\u0435 \u0441\u043c\u044b\u0441\u043b\u043e\u0432\u043e\u0439 \u043d\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0438\u0445 \u0437\u0430\u0434\u0430\u0447. \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e `stopwords` \u0438\u0437 nltk.corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e `get_tokens(words)`. \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0441\u043b\u043e\u0432. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u0441\u0441\u0438\u0432 \u0442\u043e\u043a\u0435\u043d\u043e\u0432."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tokens(text):\n",
      "    # Returns list of tokens\n",
      "\n",
      "    words = get_words(text)\n",
      "    words = [re.sub(r'(^[\\'\\-]+|[\\'\\-]+$)', '', w) for w in words]\n",
      "\n",
      "    stopwords = nltk.corpus.stopwords.words('english')\n",
      "\n",
      "    wnl = WordNetLemmatizer()\n",
      "    tokens = [wnl.lemmatize(w).lower() for w in words]\n",
      "    tokens_nonstop = [t for t in tokens if t not in stopwords]\n",
      "\n",
      "    return tokens_nonstop\n",
      "\n",
      "print get_tokens(\"here are different words\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['different', u'word']\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e `get_tweet_tokens(tweet)`. \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u0442\u0435\u043a\u0441\u0442 \u0442\u0432\u0438\u0442\u0430. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 -- \u0442\u043e\u043a\u0435\u043d\u044b \u0442\u0432\u0438\u0442\u0430. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tweet_tokens(tweet):\n",
      "    # Returns list of tweet tokens\n",
      "\n",
      "    tweet = tweet.encode('ascii', 'ignore')               # non-ascii characters\n",
      "    tweet = re.sub(r'\\bhttps?://\\S*\\b',       ' ', tweet) # urls\n",
      "    tweet = re.sub(r'@\\b\\w+\\b',               ' ', tweet) # replies\n",
      "    tweet = re.sub(r'&amp;',                  ' ', tweet) # &\n",
      "    tweet = re.sub(r'[^\\w\\-\\' ]',             ' ', tweet) # all symbols except ' and -\n",
      "    tweet = re.sub(r'\\b\\d+\\b' ,               ' ', tweet) # numbers\n",
      "    tweet = re.sub(r'\\s\\-+\\s',                ' ', tweet) # separate symbols -\n",
      "    tweet = re.sub(r'\\s\\'+\\s',                ' ', tweet) # separate symbols '\n",
      "\n",
      "    return get_tokens(tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e `collect_users_tokens()`. \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043e\u043b\u0436\u043d\u0430 \u0441\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u0412 \u044d\u0442\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u0435 \u0441\u0442\u0440\u043e\u043a\u0430 - \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c. \u0421\u0442\u043e\u043b\u0431\u0435\u0446 - \u0442\u043e\u043a\u0435\u043d. \u041d\u0430 \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0438 - \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u0442\u043e\u043a\u0435\u043d \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.\n",
      "\u0414\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c `DictVectorizer` \u0438\u0437 `sklearn.feature_extraction`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_user_dict(user_tweets):\n",
      "    # Returns dictionary of single user\n",
      "\n",
      "    tokens = [get_tweet_tokens(t) for t in user_tweets]\n",
      "    all_tokens = [word for i in tokens for word in i if word != '']\n",
      "    user_dict = dict.fromkeys(all_tokens)\n",
      "\n",
      "    for key in user_dict:\n",
      "        user_dict[key] = all_tokens.count(key)\n",
      "\n",
      "    return user_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def collect_users_tokens(df_users, tweets):\n",
      "    # Returns users list and list of user dicts. Each dict contains frequence of user tokens\n",
      "\n",
      "    return df_users[\"user_id\"], [get_user_dict(user_tweets) for user_tweets in tweets]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0444\u0430\u0439\u043b. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043c\u0435\u0442\u043e\u0434 savez \u0438\u0437 numpy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "users, users_tokens = collect_users_tokens(df_users, tweets)\n",
      "v = DictVectorizer()\n",
      "vs = v.fit_transform(users_tokens)\n",
      "\n",
      "np.savez(\"../files/out_4.dat\", data=vs, users=users, users_tokens=users_tokens)\n",
      "\n",
      "features = v.get_feature_names()\n",
      "vs_count = np.array(vs.sum(axis=0)).flatten()\n",
      "\n",
      "#num_tags = 100\n",
      "#most_popular = np.argsort(-vs_count)[:num_tags]\n",
      "#tags = [features[i] for i in most_popular]\n",
      "\n",
      "tags = dict.fromkeys(features)\n",
      "for i, f in enumerate(features):\n",
      "    tags[f] = vs_count[i]\n",
      "\n",
      "pickle.dump(tags, open(\"../files/tags.txt\", \"wb\"))\n",
      "\"\"\"\n",
      "tags = pickle.loads(open(\"../files/tags.txt\", \"rb\").read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0414\u0430\u043b\u0435\u0435 \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u0442\u043e\u043a\u0435\u043d\u0430\u0445 \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u0442\u044c \u0435\u0435 \u0432 \u0432\u0438\u0434\u0435 \u043e\u0431\u043b\u0430\u043a\u0430 \u0442\u044d\u0433\u043e\u0432. [\u041f\u043e\u0434\u0441\u043a\u0430\u0437\u043a\u0430](http://anokhin.github.io/img/tag_cloud.png). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "def draw_tag_cloud(tags):\n",
      "    # Draws tag cloud of found tokens\n",
      "\n",
      "    create_tag_image(tags, 'cloud_large.png', size=(900, 600), fontname='Lobster')\n",
      "    return\n",
      "\n",
      "#print tags\n",
      "draw_tag_cloud(tags)\n",
      "\"\"\"\n",
      "num_tags = 100\n",
      "most_popular = np.argsort(-vs_count)[:num_tags]\n",
      "tags = [features[i] for i in most_popular]\n",
      "print tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[\"i'm\", 'like', 'get', u'wa', 'one', 'day', \"it's\", 'u', 'new', 'love', 'time', 'good', \"don't\", 'go', 'know', 'today', 'lol', 'back', u'ha', 'game', 'people', 'want', 'see', 'got', 'need', 'make', 'think', 'going', 'thanks', 'really', \"can't\", 'great', 'happy', 'life', u'right', 'still', 'would', 'much', 'year', 'thing', 'via', 'come', 'work', 'look', 'video', 'best', 'wow', u'take', \"you're\", 'first', 'say', 'last', 'way', 'well', 'never', 'rt', u'week', u'guy', \"that's\", 'night', 'man', 'even', 'play', 'thank', u'hour', 'feel', 'd', 'oh', 'friend', 'please', 'better', 'next', 'live', 'warcraft', 'im', 'show', 'ever', 'always', 'follow', 'yes', 'check', \"i've\", 'shit', 'tonight', \"i'll\", 'could', 'world', 'lt', 'getting', 'watch', 'hope', 'let', 'tomorrow', 'gt', 'wait', 'god', 'home', 'girl', 'everyone', 'someone']\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u0434\u043b\u044f \u043e\u0431\u043b\u0430\u043a\u0430 \u043d\u0430\u0448\u043b\u0430 (\u0441\u043c.\u0432\u044b\u0448\u0435), \u043d\u0430\u0440\u0438\u0441\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043a\u0430 \u043d\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f :("
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}