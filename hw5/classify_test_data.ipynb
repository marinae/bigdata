{
 "metadata": {
  "name": "",
  "signature": "sha256:caf44d8be4bc11f4dfa35a733ecc0f27140c637c082dd5417343e396bb63d5cf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.feature_extraction import DictVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u044b, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0435 \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load train tokens\n",
      "train_data = np.load(\"../files/out_4.dat.npz\")\n",
      "train_tokens = train_data[\"users_tokens\"]\n",
      "\n",
      "# Create sparse matrix for train set\n",
      "dv_train = DictVectorizer()\n",
      "train_matr = dv_train.fit_transform(train_tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u044b \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load test tokens\n",
      "test_data = np.load(\"../files/out_4_test.dat.npz\")\n",
      "test_tokens = test_data[\"users_tokens\"]\n",
      "\n",
      "# Create sparse matrix for test set\n",
      "dv_test = DictVectorizer()\n",
      "dv_test.fit(train_tokens)\n",
      "test_matr = dv_test.transform(test_tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441\u044b \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438\u0437 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_users = train_data[\"users\"]\n",
      "\n",
      "TRAINING_SET = \"../files/twitter_train.txt\"\n",
      "df_users = pd.read_csv(TRAINING_SET, sep=\",\", header=0, names=[\"user_id\", \"class\"], dtype={\"user_id\": str, \"class\": str})\n",
      "df_users.set_index(\"user_id\", inplace=True)\n",
      "\n",
      "Y = df_users.ix[train_users.astype(str)][\"class\"].values\n",
      "print \"Train set: (%dx%d) feature matrix, %d target vector\" % (train_matr.shape[0], train_matr.shape[1], Y.shape[0])\n",
      "print \"Test set: (%dx%d) feature matrix\" % (test_matr.shape[0], test_matr.shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train set: (8000x294972) feature matrix, 8000 target vector\n",
        "Test set: (2000x294972) feature matrix\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0423\u0434\u0430\u043b\u0438\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0432\u0441\u0442\u0440\u0435\u0442\u0438\u043b\u0438\u0441\u044c \u043c\u0435\u043d\u0435\u0435, \u0447\u0435\u043c \u0443 100 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cut_unpopular(X):\n",
      "    \n",
      "    # Feature_array[i] contains number of users having feature[i]\n",
      "    feature_counts = np.asarray(X.astype(bool).sum(axis=0))\n",
      "    feature_array = feature_counts[0]\n",
      "\n",
      "    return feature_array\n",
      "\n",
      "col_counts = cut_unpopular(train_matr)\n",
      "X_train = train_matr.tocsc()[:, col_counts > 100].toarray()\n",
      "X_test  = test_matr.tocsc()[:, col_counts > 100].toarray()\n",
      "print \"Train set: (%dx%d) feature matrix, %d target vector\" % (X_train.shape[0], X_train.shape[1], Y.shape[0])\n",
      "print \"Test set: (%dx%d) feature matrix\" % (X_test.shape[0], X_test.shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train set: (8000x4884) feature matrix, 8000 target vector\n",
        "Test set: (2000x4884) feature matrix\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Logistic regression with L2 regularization optimized by Newton method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigm(z):\n",
      "    \n",
      "    # Sigmoid function\n",
      "    return np.clip(1.0 / (1 + np.exp(-z)), 0.000000000001, 0.999999999999)\n",
      "\n",
      "def cur_diff(w_prev, w_cur):\n",
      "    \n",
      "    # Quadratic difference between w_(n-1) and w_n\n",
      "    return np.dot(w_cur - w_prev, w_cur - w_prev)\n",
      "\n",
      "class LogisticRegression():\n",
      "    \n",
      "    def __init__(self, C=1, prec=0.1, max_iter=10):\n",
      "        \n",
      "        # L2 regularization strength\n",
      "        self.C = C\n",
      "        # Precision of weights computation\n",
      "        self.prec = prec\n",
      "        # Maximum number of iterations\n",
      "        self.max_iter = max_iter\n",
      "    \n",
      "    def fit(self, X, Y):\n",
      "        \n",
      "        # Initialize weights\n",
      "        prev_weights = np.zeros(len(X[1]) + 1)\n",
      "        prev_weights.fill(np.inf)\n",
      "        self.weights = np.zeros(len(X[1]) + 1)\n",
      "        self.weights[0] = np.log(np.mean(Y) / (1 - np.mean(Y)))\n",
      "        # Add \"constant\" column to X\n",
      "        X = np.insert(X, 0, values=1, axis=1)\n",
      "        # Current iterations count\n",
      "        it = 0\n",
      "        diff = +np.inf\n",
      "\n",
      "        # Iterate until convergence or exceeding limits\n",
      "        while (it < self.max_iter and diff > self.prec):\n",
      "            # Recompute parameters\n",
      "            eta = np.dot(X, self.weights)\n",
      "            mu = sigm(eta)\n",
      "            s = mu * (1 - mu)\n",
      "            z = eta + (Y - mu) / s\n",
      "            S = np.diag(s)\n",
      "            # Identity matrix with I[0][0] = 0\n",
      "            eye_vect = np.ones(len(X[0]))\n",
      "            eye_vect[0] = 0\n",
      "            eye_matr = np.diag(eye_vect)\n",
      "            # Recompute weights with IRLS for Newton method\n",
      "            prev_weights = self.weights\n",
      "            temp = np.dot(np.dot(X.T, S), X)\n",
      "            temp = temp + self.C * eye_matr\n",
      "            temp = np.linalg.inv(temp)\n",
      "            temp = np.dot(temp, X.T)\n",
      "            temp = np.dot(temp, S)\n",
      "            self.weights = np.dot(temp, z)\n",
      "            print self.weights\n",
      "            # Recompute difference of weights\n",
      "            diff = cur_diff(prev_weights, self.weights)\n",
      "            # Print current state\n",
      "            print \"Iteration %d: diff = %s\" % (it, diff)\n",
      "            # Increment iteration number\n",
      "            it = it + 1\n",
      "        \n",
      "        return self\n",
      "    \n",
      "    def predict_proba(self, X):\n",
      "        \n",
      "        # Add \"constant\" column to X\n",
      "        X = np.insert(X, 0, values=1, axis=1)\n",
      "        # Compute probabilities\n",
      "        eta = np.dot(X, self.weights)\n",
      "        mu = sigm(eta)\n",
      "        return mu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logreg = LogisticRegression(C=1000, max_iter=15)\n",
      "logreg.fit(X_train, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-1.09857117  0.00862266  0.00550565 ...,  0.01301114  0.00244523\n",
        "  0.00454324]\n",
        "Iteration 0: diff = 1.03645014234\n",
        "[ -1.33449009e+00   9.03492900e-03   2.98059371e-03 ...,   2.08925204e-02\n",
        "  -6.85781531e-04   3.90778589e-03]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 1: diff = 0.106692187749\n",
        "[-1.43252251  0.0098083   0.00262195 ...,  0.02394534 -0.00152813\n",
        "  0.00384433]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 2: diff = 0.035343712165\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "<__main__.LogisticRegression instance at 0x12e791f80>"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_prob = logreg.predict_proba(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u0427\u0438\u0442\u0430\u0435\u043c ID \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043a \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TEST_SET = \"../files/twitter_test.txt\"\n",
      "test_users = pd.read_csv(TEST_SET, sep=\",\", header=0, names=[\"Id\", \"Prediction\"])\n",
      "\n",
      "test_users[\"Prediction\"] = Y_prob\n",
      "OUT_FILE_PATH = \"../files/submission.csv\"\n",
      "print \"Saving submission data to %s\" % OUT_FILE_PATH\n",
      "test_users.to_csv(OUT_FILE_PATH, sep=\",\", index=False, encoding=\"utf-8\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Saving submission data to ../files/submission.csv\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}